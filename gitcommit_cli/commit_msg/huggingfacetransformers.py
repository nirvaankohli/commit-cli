from __future__ import annotations

import subprocess
from textwrap import dedent
from typing import Optional

from huggingface_hub import InferenceClient

SYSPROMPT = dedent(
    
"""\
    
You are an expert software assistant that writes high-quality Git commit messages.
Follow the Conventional Commits spec when possible.

Rules:
                       
- Output a concise subject line <= 72 chars.

- Use present tense, imperative mood.

- Infer type: feat, fix, refactor, docs, test, chore, build, ci, perf, style.

- If multiple themes exist, pick the most significant one.

- Optionally add a short body (wrapped at 72 chars) when helpful.

- Never include file headers in the subject; summarize the intent.
                       
"""

).strip()

class llm:

    """
    
    A class to create commit messages using the Hugging Face Transformers library.
    
    """

    def __init__():

        pass

    def build_prompt(
            
            self,

            diff:str,

            project_context: Optional[str] = None

    ):
        
        ctx = f"\n\nProject context:\n{project_context}" if project_context else ""

        return dedent(

            f"""\
            
            Summarize the following git diff into a Conventional Commit message.

                Requirements:
                - Capture intent and scope.
                - Mention notable APIs, migrations, flags, or configs if present.
                - 1-line subject (<=72 chars). Wrap any body at 72 columns.

            {ctx}

            Git Diff you must summarize:

            ---

            {diff.strip()}

            ---
                
            RETURN ONLY THE COMMIT MESSAGE, DO NOT RETURN ANYTHING ELSE. YOUR ONLY OUTPUT SHOULD BE THE COMMIT MESSAGE.

            YOUR ONLY JOB IS TO RETURN THE COMMIT MESSAGE, DO NOT RETURN ANYTHING ELSE.

            It should be a high-quality commit message that follows the Conventional Commits spec.

            """


        ).strip()

    def generate_commit_message(
            
            self,
             
            diff: str,

            model_name: str = "meta-llama/Llama-3.2-3B-Instruct", # MODEL

            max_tokens: int = 2000,

            temperature: float = 0.2,

            base_url: Optional[str] = None,
            
            ) -> str:

        """
        
        Generate a commit message based on the provided diff using a pre-trained model.
        
        This method uses a pre-trained model from Hugging Face Transformers to generate a commit message.
        
        Args:

            diff (str): The diff string from which to generate the commit message.

            model_name (str): The name of the pre-trained model to use. Default is "meta-llama/Llama-3.2-3B-Instruct".

            max_tokens (int): The maximum number of tokens to generate. Default is 2000.

            temperature (float): The sampling temperature to use. Default is 0.2.

            base_url (Optional[str]): Optional base URL for the Hugging Face Inference API.
        
        Returns:

            str: A commit message generated by the model.
        
        """
        
        client = InferenceClient(
            
            model=model_name, # see above
            
            base_url=base_url # see above(Most of the time this will be None)
            
            )
        
        # Create the messages for the chat completion request
        
        messages = [

            {"role": "system", "content": SYSPROMPT},

            {"role": "user", "content": self.build_prompt(diff)}

            ]
        
        # Call the chat completion endpoint of the model, supplying the messages and other parameters

        response = client.chat_completion(

            messages=messages,

            max_tokens=max_tokens,

            temperature=temperature

            )
        
        # Extract and return the generated commit message from the response

        text = response.choices[0].message.content.strip()

        # Normalize the commit message to ensure it follows the common conventions we provided

        if "\n\n" in text:

            subject, body = text.split("\n\n", 1)
            subject = subject.strip()[:72]
            body = "\n".join(line.rstrip() for line in body.splitlines()).strip()

            return f"{subject}\n\n{body}" if body else subject
        
        else:

            return text.splitlines()[0][:72].strip()

        

def get_staged_diff() -> str:

    """
    
    Reads staged changes (index).
    
    """

    out = subprocess.check_output(["git", "diff", "--staged", "--no-color"])

    return out.decode("utf-8", errors="replace").strip()


if __name__ == "__main__":

    diff = get_staged_diff()

    if not diff:

        print("chore: update (no staged changes)")

    else:
        
        model = llm()

        commit_message = model.generate_commit_message(diff)

        print(commit_message)